global:
  rbac:
    create: true
    pspEnabled: true
    pspAnnotations: {}

defaultRules:
  create: true
prometheus:
  enabled: true
  annotations: {}
  additionalServiceMonitors:
    - name: frontend-sm
      labels:
        prometheus-app: frontend-svc
      namespaceSelector:
        matchNames:
          - monitoring
      selector:
        matchLabels:
          prometheus-app: frontend-svc
      endpoints:
        - port: frontend-svc-port
          protocol: TCP
          interval: 10s
  service:
    name: frontend-svc
    namespace: monitoring
    annotations: {}
    labels:
      prometheus-app: frontend-svc
    type: LoadBalancer
    portName: frontend-svc-port
    port: 8080
    loadBalancerSourceRanges: []
  prometheusSpec:
    walCompression: true
    retention: 3d
    retentionSize: "2GiB"
    serviceMonitorNamespaceSelector:
      matchLabels:
        service-monitor-discovery: enable
grafana:
  enabled: true
  defaultDashboardsEnabled: true
  plugins:
    - pierosavi-imageit-panel
    - agenty-flowcharting-panel
  persistence:
    type: pvc
    enabled: enabled
    # storageClassName: default
    accessModes:
      - ReadWriteOnce
    size: 10Gi
    # annotations: {}
    finalizers:
      - kubernetes.io/pvc-protection
    # subPath: ""
    # existingClaim:

nodeExporter:
  serviceMonitor:
    relabelings:
      - sourceLabels: [__meta_kubernetes_pod_node_name]
        targetLabel: instance

alertmanager:
  enabled: true
  apiVersion: v2
  serviceAccount:
    create: true
    name: ""
    annotations: {}
  podDisruptionBudget:
    enabled: false
    minAvailable: 1
    maxUnavailable: ""
  config:
    global:
      resolve_timeout: 5m
    route:
      group_by: ["job"]
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: "null"
      routes:
        - match:
            alertname: Watchdog
          receiver: "null"
    receivers:
      - name: "null"
  tplConfig: false
  templateFiles: {}
  ingress:
    enabled: false
    annotations: {}
    labels: {}
    hosts: []
    paths: []
    tls: []
  secret:
    annotations: {}
  ingressPerReplica:
    enabled: false
    annotations: {}
    labels: {}
    hostPrefix: ""
    hostDomain: ""
    paths: []
    tlsSecretName: ""
    tlsSecretPerReplica:
      enabled: false
      prefix: "alertmanager"

  service:
    annotations: {}
    labels: {}
    clusterIP: ""
    port: 9093
    targetPort: 9093
    nodePort: 30903
    externalIPs: []
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    type: ClusterIP

  servicePerReplica:
    enabled: false
    annotations: {}
    port: 9093
    targetPort: 9093
    nodePort: 30904
    loadBalancerSourceRanges: []
    type: ClusterIP

  serviceMonitor:
    interval: ""
    selfMonitor: true

  alertmanagerSpec:
    nodeSelector:
      role: infrastructure
    image:
      repository: quay.io/prometheus/alertmanager
      tag: v0.21.0
      sha: ""
    useExistingSecret: false
    alertmanagerConfigSelector: {}
    alertmanagerConfigNamespaceSelector: {}
    logFormat: logfmt
    logLevel: info
    replicas: 1
    retention: 120h
    storage: {}
    routePrefix: /
    paused: false
    podAntiAffinityTopologyKey: kubernetes.io/hostname
    securityContext:
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
      fsGroup: 2000
    portName: "web"
    clusterAdvertiseAddress: false

prometheusOperator:
  enabled: true

  # If true prometheus operator will create and update its CRDs on startup
  # Only for prometheusOperator.image.tag < v0.39.0
  manageCrds: true

  tlsProxy:
    enabled: true
    image:
      repository: squareup/ghostunnel
      tag: v1.5.2
      sha: ""
      pullPolicy: IfNotPresent
    resources: {}

  ## Admission webhook support for PrometheusRules resources added in Prometheus Operator 0.30 can be enabled to prevent incorrectly formatted
  ## rules from making their way into prometheus and potentially preventing the container from starting
  admissionWebhooks:
    failurePolicy: Fail
    enabled: true
    ## If enabled, generate a self-signed certificate, then patch the webhook configurations with the generated data.
    ## On chart upgrades (or if the secret exists) the cert will not be re-generated. You can use this to provide your own
    ## certs ahead of time if you wish.
    ##
    patch:
      enabled: true
      image:
        repository: jettech/kube-webhook-certgen
        tag: v1.2.1
        sha: ""
        pullPolicy: IfNotPresent
      resources: {}
      ## Provide a priority class name to the webhook patching job
      ##
      priorityClassName: ""
      podAnnotations: {}
      nodeSelector: {}
      affinity: {}
      tolerations: []
